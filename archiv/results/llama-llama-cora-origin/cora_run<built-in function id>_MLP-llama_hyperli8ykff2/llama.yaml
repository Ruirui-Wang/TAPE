accelerator: auto
cfg_dest: llama.yaml
data:
  device: cuda:0
  include_negatives: true
  name: cora
  split_index:
  - 0.8
  - 0.15
  - 0.05
  split_labels: true
  test_pct: 0.05
  undirected: true
  val_pct: 0.15
device: cuda:0
embedder:
  type: llama
metric_best: acc
model:
  device: cuda:0
  dropout: 0.1
  hidden_channels: 128
  num_layers: 3
  type: MLP-llama
num_threads: 11
optimizer:
  base_lr: 0.0001
  type: adam
  weight_decay: 0.0005
out_dir: results/llama-llama-cora-origin
params: 541057
print: file
run:
  multiple_splits: None
  num_threads: 11
  seed: 0
run_dir: results/llama-llama-cora-origin/cora_run<built-in function id>_MLP-llama_hyperli8ykff2
run_id: 0
seed: 0
train:
  auto_resume: false
  batch_size: 1024
  device: 0
  epochs: 30
  eval_period: 1
  final_eval: false
  finetune: false
  mode: custom
wandb:
  name_tag: cora_run<built-in function id>_MLP-llama_hyperli8ykff2
  project: gtblueprint
  use: true
